{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8699558347162722\n",
      "+------+--------------------+\n",
      "|userid|     recommendations|\n",
      "+------+--------------------+\n",
      "|   471|[[3379, 4.837304]...|\n",
      "|   463|[[3379, 4.9989934...|\n",
      "|   496|[[3379, 4.867341]...|\n",
      "|   148|[[67618, 4.577209...|\n",
      "|   540|[[3379, 5.320341]...|\n",
      "|   392|[[6818, 5.365404]...|\n",
      "|   243|[[67618, 5.595224...|\n",
      "|    31|[[67618, 5.259552...|\n",
      "|   516|[[3379, 5.0185905...|\n",
      "|   580|[[60943, 5.004139...|\n",
      "|   251|[[3379, 5.903529]...|\n",
      "|   451|[[3379, 5.6381693...|\n",
      "|    85|[[1140, 4.8237133...|\n",
      "|   137|[[3379, 5.0201507...|\n",
      "|    65|[[3379, 5.0349503...|\n",
      "|   458|[[67618, 5.518743...|\n",
      "|   481|[[42730, 4.075913...|\n",
      "|    53|[[3379, 6.907212]...|\n",
      "|   255|[[1194, 4.350659]...|\n",
      "|   588|[[3379, 4.702357]...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrameReader\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "#from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# Create a SparkSession entry point.\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('NetLens') \\\n",
    "    .master('local') \\\n",
    "    .getOrCreate() \\\n",
    "    #.config(conf) \\\n",
    "\n",
    "# Connect to the penguin database, read from the ratings table\n",
    "# and store the contents into a dataframe.\n",
    "url = 'jdbc:postgresql://penguin.kent.ac.uk/ai261'\n",
    "properties = {'user': 'ai261', 'password': 'pla%boy', 'driver': 'org.postgresql.Driver'}\n",
    "df = spark.read.jdbc(url=url, table='ratings', properties=properties)\n",
    "df = df.select(['userid', 'movieid', 'rating'])\n",
    "\n",
    "# Split the dataset into a training and a test set.\n",
    "(training, test) = df.randomSplit([0.8, 0.2])\n",
    "training.cache()\n",
    "test.cache()\n",
    "\n",
    "# Create a recommendations model using ALS algorithm on the\n",
    "# training data.\n",
    "als = ALS(userCol='userid', itemCol='movieid', ratingCol='rating', rank=11,\n",
    "          maxIter=7, seed=5, regParam=0.17, coldStartStrategy='drop')\n",
    "\n",
    "# Build a parameter grid to assign a range of values to\n",
    "# the given ALS parameters.\n",
    "#output = ParamGridBuilder() \\\n",
    "#             .addGrid(als.rank, [10, 11, 12]) \\\n",
    "#             .addGrid(als.maxIter, [6, 7, 8]) \\\n",
    "#             .addGrid(als.regParam, [.16, .17, .18]) \\\n",
    "#             .addGrid(als.seed, [0, 5, 10]) \\\n",
    "#             .build()\n",
    "\n",
    "# Use the regression evaluator and fit the model to the training data.\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "#tvs = TrainValidationSplit(estimator=als, estimatorParamMaps=output, evaluator=evaluator)\n",
    "#model = tvs.fit(training)\n",
    "#tuned_model = model.bestModel\n",
    "model = als.fit(training)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Calculate the RMSE of the recommendations model on the test data.\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print('RMSE: ' + str(rmse))\n",
    "\n",
    "# Display the top 10 movies recommendations for each user.\n",
    "mr = model.recommendForAllUsers(10)\n",
    "mr.show()\n",
    "\n",
    "# Print the current ALS parameters used.\n",
    "#print('Rank: ' + str(model._java_obj.parent().getRank()))\n",
    "#print('MaxIter: ' + str(model._java_obj.parent().getMaxIter()))\n",
    "#print('RegParam: ' + str(model._java_obj.parent().getRegParam()))\n",
    "#print('Seed: ' + str(model._java_obj.parent().getSeed()))\n",
    "\n",
    "#model.save(spark, '/path/to/model')\n",
    "#load_model = MatrixFactorizationModel.load(spark, '/path/to/model')\n",
    "\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
